{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import *\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from game.api import BlackjackWrapper\n",
    "from game.game_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "class BlackjackDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that accepts a flattened state and outputs 12 values:\n",
    "    1. Q-values of bet percentages from 0.1 to 1.0 (increments of 0.1)\n",
    "    2. Q-value of taking a card (hit) or not (stand)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, bet_choices: List[float], card_choices: List[bool], epsilon: float, min_epsilon: float):\n",
    "        super().__init__()\n",
    "        self.bet_choices = bet_choices\n",
    "        self.card_choices = card_choices\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        # common layers shared by both outputs\n",
    "        self.init_layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        # layers for bet percentage Q-value output\n",
    "        self.bet_layers = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, len(bet_choices)),\n",
    "        )\n",
    "        # layers for card action Q-value output\n",
    "        self.card_layers = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, len(card_choices)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.init_layers(x)\n",
    "        x1 = self.bet_layers(x)\n",
    "        x2 = self.card_layers(x)\n",
    "        return x1, x2\n",
    "\n",
    "    def batched_forward_with_concat(self, x) -> torch.Tensor:\n",
    "        x = self.init_layers(x)\n",
    "        x1 = self.bet_layers(x)\n",
    "        x2 = self.card_layers(x)\n",
    "        return torch.cat((x1, x2), dim=-1)\n",
    "\n",
    "    def get_bet_percent(\n",
    "        self, normalized_state, allow_explore: bool, num_steps: int\n",
    "    ) -> Tuple[float, int, torch.Tensor]:\n",
    "        bet_values, card_values = self.forward(normalized_state)\n",
    "        if allow_explore and random.random() < max(self.epsilon ** num_steps, self.min_epsilon):\n",
    "            # explore\n",
    "            idx = random.randint(0, len(self.bet_choices) - 1)\n",
    "        else:\n",
    "            # exploit\n",
    "            idx = bet_values.argmax().item()\n",
    "        action = self.bet_choices[idx]\n",
    "        concat_output = torch.cat((bet_values, card_values), dim=-1).unsqueeze(0).cpu()\n",
    "        return action, idx, concat_output\n",
    "\n",
    "\n",
    "    def get_card_action(self, normalized_state, allow_explore: bool, num_steps: int) -> Tuple[bool, int, torch.Tensor]:\n",
    "        bet_values, card_values = self.forward(normalized_state)\n",
    "        if allow_explore and random.random() < max(self.epsilon ** num_steps, self.min_epsilon):\n",
    "            # explore\n",
    "            idx = random.randint(0, len(self.card_choices) - 1)\n",
    "        else:\n",
    "            # exploit\n",
    "            idx = card_values.argmax().item()\n",
    "        action = self.card_choices[idx]\n",
    "        concat_output = torch.cat((bet_values, card_values), dim=-1).unsqueeze(0).cpu()\n",
    "        return action, idx + len(self.bet_choices), concat_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer(deque):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        super().__init__([], maxlen=capacity)\n",
    "\n",
    "    def push(self, transition: Transition):\n",
    "        self.append(transition)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        return random.sample(self, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "learning_rate = 1e-3\n",
    "num_eps = 1000\n",
    "batch_size = 32\n",
    "max_steps = 1000\n",
    "\n",
    "epsilon = 0.9\n",
    "min_epsilon = 0.05\n",
    "tau = 0.005"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "bet_choices = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "card_choices = [True, False]\n",
    "\n",
    "policy_model = BlackjackDQN(\n",
    "    in_features=GameState.get_state_size(),\n",
    "    bet_choices=bet_choices,\n",
    "    card_choices=card_choices,\n",
    "    epsilon=epsilon,\n",
    "    min_epsilon=min_epsilon,\n",
    ").to(device)\n",
    "target_model = BlackjackDQN(\n",
    "    in_features=GameState.get_state_size(),\n",
    "    bet_choices=bet_choices,\n",
    "    card_choices=card_choices,\n",
    "    epsilon=epsilon,\n",
    "    min_epsilon=min_epsilon,\n",
    ").to(device)\n",
    "target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "game_wrapper = BlackjackWrapper()\n",
    "optimizer = optim.Adam(policy_model.parameters(), lr=learning_rate)\n",
    "replay_buffer = ReplayBuffer(10000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "def train(\n",
    "    policy_model: BlackjackDQN,\n",
    "    optimizer: optim.Optimizer,\n",
    "    replay_buffer: ReplayBuffer,\n",
    "    batch_size: int,\n",
    "    gamma: float,\n",
    "):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    transitions = replay_buffer.sample(batch_size)\n",
    "    # Transpose batch of transitions to get transitions with batches\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action).unsqueeze(-1)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    q_batch = policy_model.batched_forward_with_concat(state_batch)\n",
    "    state_action_values = q_batch.gather(dim=1, index=action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_model.batched_forward_with_concat(non_final_next_states).max(dim=1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_model.parameters(), 100)\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c53de6a1bdab4443be8a6d608b07c17a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\t\tRunning Average Score: 1.0\n",
      "Episode 10\t\tRunning Average Score: 1.0\n",
      "Episode 20\t\tRunning Average Score: 1.0\n",
      "Episode 30\t\tRunning Average Score: 1.0\n",
      "Episode 40\t\tRunning Average Score: 1.0\n",
      "Episode 50\t\tRunning Average Score: 1.0\n",
      "Episode 60\t\tRunning Average Score: 1.0\n",
      "Episode 70\t\tRunning Average Score: 1.0\n",
      "Episode 80\t\tRunning Average Score: 1.0\n",
      "Episode 90\t\tRunning Average Score: 1.0\n",
      "Episode 100\t\tRunning Average Score: 1.0\n",
      "Episode 110\t\tRunning Average Score: 1.0\n",
      "Episode 120\t\tRunning Average Score: 1.0\n",
      "Episode 130\t\tRunning Average Score: 1.0\n",
      "Episode 140\t\tRunning Average Score: 1.0\n",
      "Episode 150\t\tRunning Average Score: 1.0\n",
      "Episode 160\t\tRunning Average Score: 1.0\n",
      "Episode 170\t\tRunning Average Score: 1.0\n",
      "Episode 180\t\tRunning Average Score: 1.0\n",
      "Episode 190\t\tRunning Average Score: 1.0\n",
      "Episode 200\t\tRunning Average Score: 1.0\n",
      "Episode 210\t\tRunning Average Score: 1.0\n",
      "Episode 220\t\tRunning Average Score: 1.0\n",
      "Episode 230\t\tRunning Average Score: 1.0\n",
      "Episode 240\t\tRunning Average Score: 1.0\n",
      "Episode 250\t\tRunning Average Score: 1.0\n",
      "Episode 260\t\tRunning Average Score: 1.0\n",
      "Episode 270\t\tRunning Average Score: 1.0\n",
      "Episode 280\t\tRunning Average Score: 1.0\n",
      "Episode 290\t\tRunning Average Score: 1.0\n",
      "Episode 300\t\tRunning Average Score: 1.0\n",
      "Episode 310\t\tRunning Average Score: 1.0\n",
      "Episode 320\t\tRunning Average Score: 1.0\n",
      "Episode 330\t\tRunning Average Score: 1.0\n",
      "Episode 340\t\tRunning Average Score: 1.0\n",
      "Episode 350\t\tRunning Average Score: 1.0\n",
      "Episode 360\t\tRunning Average Score: 1.0\n",
      "Episode 370\t\tRunning Average Score: 1.0\n",
      "Episode 380\t\tRunning Average Score: 1.0\n",
      "Episode 390\t\tRunning Average Score: 1.0\n",
      "Episode 400\t\tRunning Average Score: 1.0\n",
      "Episode 410\t\tRunning Average Score: 1.0\n",
      "Episode 420\t\tRunning Average Score: 1.0\n",
      "Episode 430\t\tRunning Average Score: 1.0\n",
      "Episode 440\t\tRunning Average Score: 1.0\n",
      "Episode 450\t\tRunning Average Score: 1.0\n",
      "Episode 460\t\tRunning Average Score: 1.0\n",
      "Episode 470\t\tRunning Average Score: 1.0\n",
      "Episode 480\t\tRunning Average Score: 1.0\n",
      "Episode 490\t\tRunning Average Score: 1.0\n",
      "Episode 500\t\tRunning Average Score: 1.0\n",
      "Episode 510\t\tRunning Average Score: 1.0\n",
      "Episode 520\t\tRunning Average Score: 1.0\n",
      "Episode 530\t\tRunning Average Score: 1.0\n",
      "Episode 540\t\tRunning Average Score: 1.0\n",
      "Episode 550\t\tRunning Average Score: 1.0\n",
      "Episode 560\t\tRunning Average Score: 1.0\n",
      "Episode 570\t\tRunning Average Score: 1.0\n",
      "Episode 580\t\tRunning Average Score: 1.0\n",
      "Episode 590\t\tRunning Average Score: 1.0\n",
      "Episode 600\t\tRunning Average Score: 1.0\n",
      "Episode 610\t\tRunning Average Score: 1.0\n",
      "Episode 620\t\tRunning Average Score: 1.0\n",
      "Episode 630\t\tRunning Average Score: 1.0\n",
      "Episode 640\t\tRunning Average Score: 1.0\n",
      "Episode 650\t\tRunning Average Score: 1.0\n",
      "Episode 660\t\tRunning Average Score: 1.0\n",
      "Episode 670\t\tRunning Average Score: 1.0\n",
      "Episode 680\t\tRunning Average Score: 1.0\n",
      "Episode 690\t\tRunning Average Score: 1.0\n",
      "Episode 700\t\tRunning Average Score: 1.0\n",
      "Episode 710\t\tRunning Average Score: 1.0\n",
      "Episode 720\t\tRunning Average Score: 1.0\n",
      "Episode 730\t\tRunning Average Score: 1.0\n",
      "Episode 740\t\tRunning Average Score: 1.0\n",
      "Episode 750\t\tRunning Average Score: 1.0\n",
      "Episode 760\t\tRunning Average Score: 1.0\n",
      "Episode 770\t\tRunning Average Score: 1.0\n",
      "Episode 780\t\tRunning Average Score: 1.0\n",
      "Episode 790\t\tRunning Average Score: 1.0\n",
      "Episode 800\t\tRunning Average Score: 1.0\n",
      "Episode 810\t\tRunning Average Score: 1.0\n",
      "Episode 820\t\tRunning Average Score: 1.0\n",
      "Episode 830\t\tRunning Average Score: 1.0\n",
      "Episode 840\t\tRunning Average Score: 1.0\n",
      "Episode 850\t\tRunning Average Score: 1.0\n",
      "Episode 860\t\tRunning Average Score: 1.0\n",
      "Episode 870\t\tRunning Average Score: 1.0\n",
      "Episode 880\t\tRunning Average Score: 1.0\n",
      "Episode 890\t\tRunning Average Score: 1.0\n",
      "Episode 900\t\tRunning Average Score: 1.0\n",
      "Episode 910\t\tRunning Average Score: 1.0\n",
      "Episode 920\t\tRunning Average Score: 1.0\n",
      "Episode 930\t\tRunning Average Score: 1.0\n",
      "Episode 940\t\tRunning Average Score: 1.0\n",
      "Episode 950\t\tRunning Average Score: 1.0\n",
      "Episode 960\t\tRunning Average Score: 1.0\n",
      "Episode 970\t\tRunning Average Score: 1.0\n",
      "Episode 980\t\tRunning Average Score: 1.0\n",
      "Episode 990\t\tRunning Average Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "\n",
    "for i_episode in trange(num_eps):\n",
    "    game_wrapper.reset()\n",
    "    game_state = game_wrapper.get_state()\n",
    "    state = game_state.torch_flatten(device)\n",
    "    for i_step in range(max_steps):\n",
    "        if i_step == 0:\n",
    "            bet_percent, action, q_values = policy_model.get_bet_percent(\n",
    "                normalized_state=state, allow_explore=True, num_steps=i_episode\n",
    "            )\n",
    "            outcome = game_wrapper.bet_step(bet_percent)\n",
    "        else:\n",
    "            card_action, action, q_values = policy_model.get_card_action(\n",
    "                normalized_state=state, allow_explore=True, num_steps=i_episode\n",
    "            )\n",
    "            outcome = game_wrapper.card_step(take_card=card_action)\n",
    "        terminated = outcome.terminated\n",
    "        reward = outcome.reward\n",
    "        reward_tensor = torch.Tensor([reward], device=device)\n",
    "        action_tensor = torch.Tensor([action], device=device).type(torch.int64)\n",
    "        total_reward += reward\n",
    "        next_state = (\n",
    "            outcome.new_state.torch_flatten(device)\n",
    "            if not terminated else None\n",
    "        )\n",
    "\n",
    "        # Store the transition in memory\n",
    "        replay_buffer.push(Transition(state, action_tensor, next_state, reward_tensor))\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        train(\n",
    "            policy_model=policy_model,\n",
    "            optimizer=optimizer,\n",
    "            replay_buffer=replay_buffer,\n",
    "            batch_size=batch_size,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "        # Update target model to weighted sum of policy and target model\n",
    "        target_state_dict = target_model.state_dict()\n",
    "        policy_state_dict = policy_model.state_dict()\n",
    "        for key in target_state_dict:\n",
    "            target_state_dict[key] = tau * policy_state_dict[key] + (1 - tau) * target_state_dict[key]\n",
    "        target_model.load_state_dict(target_state_dict)\n",
    "\n",
    "        if terminated:\n",
    "            break\n",
    "\n",
    "    if i_episode % (num_eps // 100) == 0:\n",
    "        tqdm.write(f\"Episode {i_episode}\\t\\tRunning Average Score: {round(total_reward / (i_episode + 1), 3)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"blackjack_dqn\"\n",
    "proj_path = os.path.join(os.getcwd(), \"..\")\n",
    "model_path = os.path.join(proj_path, \"models\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
