{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import *\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as distributions\n",
    "\n",
    "from game.api import BlackjackWrapper\n",
    "from game.game_models import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class BlackjackPolicyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that accepts a flattened state and outputs 2 values:\n",
    "    1. Bet percentage from 0 to 1\n",
    "    2. Probability of taking a card (hit) from 0 to 1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        # common layers shared by both outputs\n",
    "        self.init_layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        # layers for bet percentage output\n",
    "        self.bet_layers = nn.Sequential(\n",
    "            nn.Linear(32, 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        # layers for card action output\n",
    "        self.card_layers = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.init_layers(x)\n",
    "        x1 = self.bet_layers(x)\n",
    "        x2 = self.card_layers(x)\n",
    "        return x1, x2\n",
    "\n",
    "    def get_bet_percent(self, normalized_state) -> torch.Tensor:\n",
    "        state = torch.from_numpy(normalized_state).float().unsqueeze(0).to(device)\n",
    "        bet_percent, _ = self.forward(state)\n",
    "        return bet_percent.cpu()\n",
    "\n",
    "    def get_card_action(self, normalized_state) -> torch.Tensor:\n",
    "        state = torch.from_numpy(normalized_state).float().unsqueeze(0).to(device)\n",
    "        _, card_prob = self.forward(state)\n",
    "        return card_prob.cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def reinforce(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    optimizer: optim.Optimizer,\n",
    "    num_eps: int,\n",
    "    gamma: float,\n",
    "    max_steps: int = 1000,\n",
    "    log_every: int = 10,\n",
    "):\n",
    "    print(\"Starting RL training process...\")\n",
    "    eps_scores: List[float] = []\n",
    "\n",
    "    for i_eps in trange(num_eps):\n",
    "        saved_outputs = []\n",
    "        rewards: List[float] = []\n",
    "        game_wrapper.reset()\n",
    "        state = game_wrapper.get_state()\n",
    "\n",
    "        for i_step in range(max_steps):\n",
    "            if i_step == 0:\n",
    "                bet_percent = policy_model.get_bet_percent(state.flatten())\n",
    "                saved_outputs.append(bet_percent)\n",
    "                outcome = game_wrapper.bet_step(bet_percent)\n",
    "            else:\n",
    "                card_action = policy_model.get_card_action(state.flatten())\n",
    "                saved_outputs.append(card_action)\n",
    "                outcome = game_wrapper.card_step(take_card=card_action.item() > random.random())\n",
    "            state = outcome.new_state\n",
    "            terminated = outcome.terminated\n",
    "            rewards.append(outcome.reward)\n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "        n_steps = len(rewards)\n",
    "        eps_scores.append(sum(rewards))\n",
    "        returns = deque(maxlen=n_steps)\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = returns[0] if len(returns) > 0 else 0\n",
    "            returns.appendleft(gamma * disc_return_t + rewards[t])\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        # normalize returns\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        model_loss_arr = []\n",
    "        for output, pred_return in zip(saved_outputs, returns):\n",
    "            model_loss_arr.append(-output * pred_return)\n",
    "        model_loss = torch.cat(model_loss_arr).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_eps % log_every == 0:\n",
    "            tqdm.write(f\"Episode {i_eps}\\t\\tRunning Average Score: {round(np.mean(eps_scores).item(), 3)}\")\n",
    "\n",
    "    return eps_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "learning_rate = 1e-3\n",
    "num_eps = 1000\n",
    "log_eps = int(num_eps / 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "game_wrapper = BlackjackWrapper()\n",
    "model = BlackjackPolicyModel(in_features=GameState.get_state_size())\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "model_name = \"blackjack_policy_model\"\n",
    "proj_path = os.path.join(os.getcwd(), \"..\")\n",
    "model_path = os.path.join(proj_path, \"models\", model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RL training process...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb132fb8d1f249c588a3396bbb6bf61a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\t\tRunning Average Score: 1.0\n",
      "Episode 10\t\tRunning Average Score: 1.0\n",
      "Episode 20\t\tRunning Average Score: 1.0\n",
      "Episode 30\t\tRunning Average Score: 1.0\n",
      "Episode 40\t\tRunning Average Score: 1.0\n",
      "Episode 50\t\tRunning Average Score: 1.0\n",
      "Episode 60\t\tRunning Average Score: 1.0\n",
      "Episode 70\t\tRunning Average Score: 1.0\n",
      "Episode 80\t\tRunning Average Score: 1.0\n",
      "Episode 90\t\tRunning Average Score: 1.0\n",
      "Episode 100\t\tRunning Average Score: 1.0\n",
      "Episode 110\t\tRunning Average Score: 1.0\n",
      "Episode 120\t\tRunning Average Score: 1.0\n",
      "Episode 130\t\tRunning Average Score: 1.0\n",
      "Episode 140\t\tRunning Average Score: 1.0\n",
      "Episode 150\t\tRunning Average Score: 1.0\n",
      "Episode 160\t\tRunning Average Score: 1.0\n",
      "Episode 170\t\tRunning Average Score: 1.0\n",
      "Episode 180\t\tRunning Average Score: 1.0\n",
      "Episode 190\t\tRunning Average Score: 1.0\n",
      "Episode 200\t\tRunning Average Score: 1.0\n",
      "Episode 210\t\tRunning Average Score: 1.0\n",
      "Episode 220\t\tRunning Average Score: 1.0\n",
      "Episode 230\t\tRunning Average Score: 1.0\n",
      "Episode 240\t\tRunning Average Score: 1.0\n",
      "Episode 250\t\tRunning Average Score: 1.0\n",
      "Episode 260\t\tRunning Average Score: 1.0\n",
      "Episode 270\t\tRunning Average Score: 1.0\n",
      "Episode 280\t\tRunning Average Score: 1.0\n",
      "Episode 290\t\tRunning Average Score: 1.0\n",
      "Episode 300\t\tRunning Average Score: 1.0\n",
      "Episode 310\t\tRunning Average Score: 1.0\n",
      "Episode 320\t\tRunning Average Score: 1.0\n",
      "Episode 330\t\tRunning Average Score: 1.0\n",
      "Episode 340\t\tRunning Average Score: 1.0\n",
      "Episode 350\t\tRunning Average Score: 1.0\n",
      "Episode 360\t\tRunning Average Score: 1.0\n",
      "Episode 370\t\tRunning Average Score: 1.0\n",
      "Episode 380\t\tRunning Average Score: 1.0\n",
      "Episode 390\t\tRunning Average Score: 1.0\n",
      "Episode 400\t\tRunning Average Score: 1.0\n",
      "Episode 410\t\tRunning Average Score: 1.0\n",
      "Episode 420\t\tRunning Average Score: 1.0\n",
      "Episode 430\t\tRunning Average Score: 1.0\n",
      "Episode 440\t\tRunning Average Score: 1.0\n",
      "Episode 450\t\tRunning Average Score: 1.0\n",
      "Episode 460\t\tRunning Average Score: 1.0\n",
      "Episode 470\t\tRunning Average Score: 1.0\n",
      "Episode 480\t\tRunning Average Score: 1.0\n",
      "Episode 490\t\tRunning Average Score: 1.0\n",
      "Episode 500\t\tRunning Average Score: 1.0\n",
      "Episode 510\t\tRunning Average Score: 1.0\n",
      "Episode 520\t\tRunning Average Score: 1.0\n",
      "Episode 530\t\tRunning Average Score: 1.0\n",
      "Episode 540\t\tRunning Average Score: 1.0\n",
      "Episode 550\t\tRunning Average Score: 1.0\n",
      "Episode 560\t\tRunning Average Score: 1.0\n",
      "Episode 570\t\tRunning Average Score: 1.0\n",
      "Episode 580\t\tRunning Average Score: 1.0\n",
      "Episode 590\t\tRunning Average Score: 1.0\n",
      "Episode 600\t\tRunning Average Score: 1.0\n",
      "Episode 610\t\tRunning Average Score: 1.0\n",
      "Episode 620\t\tRunning Average Score: 1.0\n",
      "Episode 630\t\tRunning Average Score: 1.0\n",
      "Episode 640\t\tRunning Average Score: 1.0\n",
      "Episode 650\t\tRunning Average Score: 1.0\n",
      "Episode 660\t\tRunning Average Score: 1.0\n",
      "Episode 670\t\tRunning Average Score: 1.0\n",
      "Episode 680\t\tRunning Average Score: 1.0\n",
      "Episode 690\t\tRunning Average Score: 1.0\n",
      "Episode 700\t\tRunning Average Score: 1.0\n",
      "Episode 710\t\tRunning Average Score: 1.0\n",
      "Episode 720\t\tRunning Average Score: 1.0\n",
      "Episode 730\t\tRunning Average Score: 1.0\n",
      "Episode 740\t\tRunning Average Score: 1.0\n",
      "Episode 750\t\tRunning Average Score: 1.0\n",
      "Episode 760\t\tRunning Average Score: 1.0\n",
      "Episode 770\t\tRunning Average Score: 1.0\n",
      "Episode 780\t\tRunning Average Score: 1.0\n",
      "Episode 790\t\tRunning Average Score: 1.0\n",
      "Episode 800\t\tRunning Average Score: 1.0\n",
      "Episode 810\t\tRunning Average Score: 1.0\n",
      "Episode 820\t\tRunning Average Score: 1.0\n",
      "Episode 830\t\tRunning Average Score: 1.0\n",
      "Episode 840\t\tRunning Average Score: 1.0\n",
      "Episode 850\t\tRunning Average Score: 1.0\n",
      "Episode 860\t\tRunning Average Score: 1.0\n",
      "Episode 870\t\tRunning Average Score: 1.0\n",
      "Episode 880\t\tRunning Average Score: 1.0\n",
      "Episode 890\t\tRunning Average Score: 1.0\n",
      "Episode 900\t\tRunning Average Score: 1.0\n",
      "Episode 910\t\tRunning Average Score: 1.0\n",
      "Episode 920\t\tRunning Average Score: 1.0\n",
      "Episode 930\t\tRunning Average Score: 1.0\n",
      "Episode 940\t\tRunning Average Score: 1.0\n",
      "Episode 950\t\tRunning Average Score: 1.0\n",
      "Episode 960\t\tRunning Average Score: 1.0\n",
      "Episode 970\t\tRunning Average Score: 1.0\n",
      "Episode 980\t\tRunning Average Score: 1.0\n",
      "Episode 990\t\tRunning Average Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(\n",
    "    game_wrapper=game_wrapper,\n",
    "    policy_model=model,\n",
    "    optimizer=optimizer,\n",
    "    num_eps=num_eps,\n",
    "    gamma=gamma,\n",
    "    log_every=log_eps,\n",
    ")\n",
    "torch.save(model, model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def evaluate_agent(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    num_eps: int,\n",
    "    max_steps: int = 1000,\n",
    ") -> Tuple[float, float]:\n",
    "    rewards: List[float] = []\n",
    "\n",
    "    for i_eps in range(num_eps):\n",
    "        game_wrapper.reset()\n",
    "        state = game_wrapper.get_state()\n",
    "        eps_reward = 0.0\n",
    "        for i_step in range(max_steps):\n",
    "            if i_step == 0:\n",
    "                bet_percent = policy_model.get_bet_percent(state.flatten())\n",
    "                outcome = game_wrapper.bet_step(bet_percent)\n",
    "            else:\n",
    "                card_action = policy_model.get_card_action(state.flatten())\n",
    "                outcome = game_wrapper.card_step(take_card=card_action.item() > random.random())\n",
    "            state = outcome.new_state\n",
    "            terminated = outcome.terminated\n",
    "            eps_reward += outcome.reward\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(eps_reward)\n",
    "\n",
    "    mean_reward = np.mean(rewards).item()\n",
    "    std_reward = np.std(rewards).item()\n",
    "    return mean_reward, std_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(1.0, 0.0)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent(\n",
    "    game_wrapper=game_wrapper,\n",
    "    policy_model=model,\n",
    "    num_eps=1000,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
