{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import *\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as distributions\n",
    "\n",
    "from game.api import BlackjackWrapper\n",
    "from game.game_models import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class BlackjackPolicyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that accepts a flattened state and outputs 2 values:\n",
    "    1. Bet percentage from 0 to 1\n",
    "    2. Probability of taking a card (hit) from 0 to 1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        # common layers shared by both outputs\n",
    "        self.init_layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Linear(64, 16),\n",
    "        )\n",
    "        # layers for bet percentage output\n",
    "        self.bet_layers = nn.Sequential(\n",
    "            nn.Linear(16, 4),\n",
    "            nn.Linear(4, 1),\n",
    "        )\n",
    "        self.bet_act = nn.Sigmoid()\n",
    "        # layers for card action output\n",
    "        self.card_layers = nn.Sequential(\n",
    "            nn.Linear(16, 8),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Linear(4, 1),\n",
    "        )\n",
    "        self.card_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.init_layers(x)\n",
    "        x1 = self.bet_layers(x)\n",
    "        x1 = self.bet_act(x1)\n",
    "        x2 = self.card_layers(x)\n",
    "        x2 = self.card_act(x2)\n",
    "        return x1, x2\n",
    "\n",
    "    def get_bet_percent(self, normalized_state) -> torch.Tensor:\n",
    "        state = torch.from_numpy(normalized_state).float().unsqueeze(0).to(device)\n",
    "        bet_percent, _ = self.forward(state)\n",
    "        return bet_percent.cpu()\n",
    "\n",
    "    def get_card_action(self, normalized_state) -> torch.Tensor:\n",
    "        state = torch.from_numpy(normalized_state).float().unsqueeze(0).to(device)\n",
    "        _, card_prob = self.forward(state)\n",
    "        return card_prob.cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def reinforce(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    optimizer: optim.Optimizer,\n",
    "    num_eps: int,\n",
    "    gamma: float,\n",
    "    max_steps: int = 1000,\n",
    "    log_every: int = 10,\n",
    "):\n",
    "    print(\"Starting RL training process...\")\n",
    "    eps_scores: List[float] = []\n",
    "\n",
    "    for i_eps in trange(num_eps):\n",
    "        saved_outputs = []\n",
    "        rewards: List[float] = []\n",
    "        game_wrapper.reset()\n",
    "        state = game_wrapper.get_state()\n",
    "\n",
    "        for i_step in range(max_steps):\n",
    "            if i_step == 0:\n",
    "                bet_percent = policy_model.get_bet_percent(state.flatten())\n",
    "                saved_outputs.append(bet_percent)\n",
    "                outcome = game_wrapper.bet_step(bet_percent)\n",
    "            else:\n",
    "                card_action = policy_model.get_card_action(state.flatten())\n",
    "                saved_outputs.append(card_action)\n",
    "                outcome = game_wrapper.card_step(take_card=card_action.item() > random.random())\n",
    "            state = outcome.new_state\n",
    "            terminated = outcome.terminated\n",
    "            rewards.append(outcome.reward)\n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "        n_steps = len(rewards)\n",
    "        eps_scores.append(sum(rewards))\n",
    "        returns = deque(maxlen=n_steps)\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = returns[0] if len(returns) > 0 else 0\n",
    "            returns.appendleft(gamma * disc_return_t + rewards[t])\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        # normalize returns\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        model_loss_arr = []\n",
    "        for output, pred_return in zip(saved_outputs, returns):\n",
    "            model_loss_arr.append(-output * pred_return)\n",
    "        model_loss = torch.cat(model_loss_arr).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_eps % log_every == 0:\n",
    "            tqdm.write(f\"Episode {i_eps}\\t\\tRunning Average Score: {round(np.mean(eps_scores).item(), 3)}\")\n",
    "\n",
    "    return eps_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "learning_rate = 1e-3\n",
    "num_eps = 100\n",
    "log_eps = int(num_eps / 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "game_wrapper = BlackjackWrapper()\n",
    "model = BlackjackPolicyModel(in_features=GameState.get_state_size())\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RL training process...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "adbe44e27a2049ba97145714bffafa29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\t\tRunning Average Score: 1.0\n",
      "Episode 1\t\tRunning Average Score: 1.0\n",
      "Episode 2\t\tRunning Average Score: 1.0\n",
      "Episode 3\t\tRunning Average Score: 1.0\n",
      "Episode 4\t\tRunning Average Score: 1.0\n",
      "Episode 5\t\tRunning Average Score: 1.0\n",
      "Episode 6\t\tRunning Average Score: 1.0\n",
      "Episode 7\t\tRunning Average Score: 1.0\n",
      "Episode 8\t\tRunning Average Score: 1.0\n",
      "Episode 9\t\tRunning Average Score: 1.0\n",
      "Episode 10\t\tRunning Average Score: 1.0\n",
      "Episode 11\t\tRunning Average Score: 1.0\n",
      "Episode 12\t\tRunning Average Score: 1.0\n",
      "Episode 13\t\tRunning Average Score: 1.0\n",
      "Episode 14\t\tRunning Average Score: 1.0\n",
      "Episode 15\t\tRunning Average Score: 1.0\n",
      "Episode 16\t\tRunning Average Score: 1.0\n",
      "Episode 17\t\tRunning Average Score: 1.0\n",
      "Episode 18\t\tRunning Average Score: 1.0\n",
      "Episode 19\t\tRunning Average Score: 1.0\n",
      "Episode 20\t\tRunning Average Score: 1.0\n",
      "Episode 21\t\tRunning Average Score: 1.0\n",
      "Episode 22\t\tRunning Average Score: 1.0\n",
      "Episode 23\t\tRunning Average Score: 1.0\n",
      "Episode 24\t\tRunning Average Score: 1.0\n",
      "Episode 25\t\tRunning Average Score: 1.0\n",
      "Episode 26\t\tRunning Average Score: 1.0\n",
      "Episode 27\t\tRunning Average Score: 1.0\n",
      "Episode 28\t\tRunning Average Score: 1.0\n",
      "Episode 29\t\tRunning Average Score: 1.0\n",
      "Episode 30\t\tRunning Average Score: 1.0\n",
      "Episode 31\t\tRunning Average Score: 1.0\n",
      "Episode 32\t\tRunning Average Score: 1.0\n",
      "Episode 33\t\tRunning Average Score: 1.0\n",
      "Episode 34\t\tRunning Average Score: 1.0\n",
      "Episode 35\t\tRunning Average Score: 1.0\n",
      "Episode 36\t\tRunning Average Score: 1.0\n",
      "Episode 37\t\tRunning Average Score: 1.0\n",
      "Episode 38\t\tRunning Average Score: 1.0\n",
      "Episode 39\t\tRunning Average Score: 1.0\n",
      "Episode 40\t\tRunning Average Score: 1.0\n",
      "Episode 41\t\tRunning Average Score: 1.0\n",
      "Episode 42\t\tRunning Average Score: 1.0\n",
      "Episode 43\t\tRunning Average Score: 1.0\n",
      "Episode 44\t\tRunning Average Score: 1.0\n",
      "Episode 45\t\tRunning Average Score: 1.0\n",
      "Episode 46\t\tRunning Average Score: 1.0\n",
      "Episode 47\t\tRunning Average Score: 1.0\n",
      "Episode 48\t\tRunning Average Score: 1.0\n",
      "Episode 49\t\tRunning Average Score: 1.0\n",
      "Episode 50\t\tRunning Average Score: 1.0\n",
      "Episode 51\t\tRunning Average Score: 1.0\n",
      "Episode 52\t\tRunning Average Score: 1.0\n",
      "Episode 53\t\tRunning Average Score: 1.0\n",
      "Episode 54\t\tRunning Average Score: 1.0\n",
      "Episode 55\t\tRunning Average Score: 1.0\n",
      "Episode 56\t\tRunning Average Score: 1.0\n",
      "Episode 57\t\tRunning Average Score: 1.0\n",
      "Episode 58\t\tRunning Average Score: 1.0\n",
      "Episode 59\t\tRunning Average Score: 1.0\n",
      "Episode 60\t\tRunning Average Score: 1.0\n",
      "Episode 61\t\tRunning Average Score: 1.0\n",
      "Episode 62\t\tRunning Average Score: 1.0\n",
      "Episode 63\t\tRunning Average Score: 1.0\n",
      "Episode 64\t\tRunning Average Score: 1.0\n",
      "Episode 65\t\tRunning Average Score: 1.0\n",
      "Episode 66\t\tRunning Average Score: 1.0\n",
      "Episode 67\t\tRunning Average Score: 1.0\n",
      "Episode 68\t\tRunning Average Score: 1.0\n",
      "Episode 69\t\tRunning Average Score: 1.0\n",
      "Episode 70\t\tRunning Average Score: 1.0\n",
      "Episode 71\t\tRunning Average Score: 1.0\n",
      "Episode 72\t\tRunning Average Score: 1.0\n",
      "Episode 73\t\tRunning Average Score: 1.0\n",
      "Episode 74\t\tRunning Average Score: 1.0\n",
      "Episode 75\t\tRunning Average Score: 1.0\n",
      "Episode 76\t\tRunning Average Score: 1.0\n",
      "Episode 77\t\tRunning Average Score: 1.0\n",
      "Episode 78\t\tRunning Average Score: 1.0\n",
      "Episode 79\t\tRunning Average Score: 1.0\n",
      "Episode 80\t\tRunning Average Score: 1.0\n",
      "Episode 81\t\tRunning Average Score: 1.0\n",
      "Episode 82\t\tRunning Average Score: 1.0\n",
      "Episode 83\t\tRunning Average Score: 1.0\n",
      "Episode 84\t\tRunning Average Score: 1.0\n",
      "Episode 85\t\tRunning Average Score: 1.0\n",
      "Episode 86\t\tRunning Average Score: 1.0\n",
      "Episode 87\t\tRunning Average Score: 1.0\n",
      "Episode 88\t\tRunning Average Score: 1.0\n",
      "Episode 89\t\tRunning Average Score: 1.0\n",
      "Episode 90\t\tRunning Average Score: 1.0\n",
      "Episode 91\t\tRunning Average Score: 1.0\n",
      "Episode 92\t\tRunning Average Score: 1.0\n",
      "Episode 93\t\tRunning Average Score: 1.0\n",
      "Episode 94\t\tRunning Average Score: 1.0\n",
      "Episode 95\t\tRunning Average Score: 1.0\n",
      "Episode 96\t\tRunning Average Score: 1.0\n",
      "Episode 97\t\tRunning Average Score: 1.0\n",
      "Episode 98\t\tRunning Average Score: 1.0\n",
      "Episode 99\t\tRunning Average Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(\n",
    "    game_wrapper=game_wrapper,\n",
    "    policy_model=model,\n",
    "    optimizer=optimizer,\n",
    "    num_eps=num_eps,\n",
    "    gamma=gamma,\n",
    "    log_every=log_eps,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_agent(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    num_eps: int,\n",
    "    max_steps: int = 1000,\n",
    ") -> Tuple[float, float]:\n",
    "    rewards: List[float] = []\n",
    "\n",
    "    for i_eps in range(num_eps):\n",
    "        game_wrapper.reset()\n",
    "        state = game_wrapper.get_state()\n",
    "        eps_reward = 0.0\n",
    "        for i_step in range(max_steps):\n",
    "            if i_step == 0:\n",
    "                bet_percent = policy_model.get_bet_percent(state.flatten())\n",
    "                outcome = game_wrapper.bet_step(bet_percent)\n",
    "            else:\n",
    "                card_action = policy_model.get_card_action(state.flatten())\n",
    "                outcome = game_wrapper.card_step(take_card=card_action.item() > random.random())\n",
    "            state = outcome.new_state\n",
    "            terminated = outcome.terminated\n",
    "            eps_reward += outcome.reward\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(eps_reward)\n",
    "\n",
    "    mean_reward = np.mean(rewards).item()\n",
    "    std_reward = np.std(rewards).item()\n",
    "    return mean_reward, std_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
