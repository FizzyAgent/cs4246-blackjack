{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import *\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from game.api import BlackjackWrapper\n",
    "from game.models.model import GameState\n",
    "from training.agent import BlackjackPolicyModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reinforce(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler: optim.lr_scheduler._LRScheduler,\n",
    "    scheduler_step_every: int,\n",
    "    num_eps: int,\n",
    "    gamma: float,\n",
    "    max_steps: int = 1000,\n",
    "    log_every: int = 10,\n",
    "):\n",
    "    print(\"Starting RL training process...\")\n",
    "    eps_scores: List[float] = []\n",
    "    last_logged_eps_scores: List[float] = []\n",
    "\n",
    "    for i_eps in trange(num_eps):\n",
    "        outputs = []\n",
    "        expected_outputs = []\n",
    "        rewards: List[float] = []\n",
    "        game_wrapper = game_wrapper.reset()\n",
    "        state = game_wrapper.get_state()\n",
    "\n",
    "        for i_step in range(max_steps):\n",
    "            if i_step == 0:\n",
    "                bet_percent = policy_model.get_bet_percent(state.flatten())\n",
    "                outcome = game_wrapper.bet_step(bet_percent)\n",
    "                outputs.append(bet_percent)\n",
    "            else:\n",
    "                card_action = policy_model.get_card_action(state.flatten())\n",
    "                outputs.append(card_action)\n",
    "                take_card = card_action.item() > random.random()\n",
    "                outcome = game_wrapper.card_step(take_card=take_card)\n",
    "            state = outcome.new_state\n",
    "            terminated = outcome.terminated\n",
    "            rewards.append(outcome.reward)\n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "        n_steps = len(rewards)\n",
    "        eps_reward = sum(rewards)\n",
    "        eps_scores.append(eps_reward)\n",
    "        last_logged_eps_scores.append(eps_reward)\n",
    "        returns = deque(maxlen=n_steps)\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = returns[0] if len(returns) > 0 else 0\n",
    "            returns.appendleft(gamma * disc_return_t + rewards[t])\n",
    "        returns = torch.tensor(returns)\n",
    "\n",
    "        model_loss_arr = []\n",
    "        for output, pred_return in zip(outputs, returns):\n",
    "            model_loss_arr.append(-pred_return * (output + 1e-8).log())\n",
    "        model_loss = torch.cat(model_loss_arr).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_eps % scheduler_step_every == 0:\n",
    "            scheduler.step()\n",
    "\n",
    "        if i_eps % log_every == 0:\n",
    "            tqdm.write(\n",
    "                f\"Episode {i_eps}\"\n",
    "                f\"\\t\\tLast Logged Average Score: {round(np.mean(last_logged_eps_scores).item(), 3)}\"\n",
    "                f\"\\t\\tRunning Average Score: {round(np.mean(eps_scores).item(), 3)}\"\n",
    "            )\n",
    "            last_logged_eps_scores = []\n",
    "\n",
    "    return eps_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gamma = 0.75\n",
    "learning_rate = 1e-2\n",
    "num_eps = 10_000\n",
    "lr_gamma = 0.99\n",
    "num_lr_decay = 100\n",
    "log_eps = int(num_eps / 100)\n",
    "\n",
    "initial_cash = 1000\n",
    "deck_num = 8\n",
    "min_bet = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "game_wrapper = BlackjackWrapper(initial_cash, deck_num, min_bet)\n",
    "model = BlackjackPolicyModel(in_features=GameState.get_state_size(), device=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_gamma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"blackjack_policy_model\"\n",
    "proj_path = os.path.join(os.getcwd(), \"..\")\n",
    "model_path = os.path.join(proj_path, \"models\", model_name)\n",
    "scores_path = os.path.join(proj_path, \"models\", f\"{model_name}_training_scores\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train()\n",
    "scores = reinforce(\n",
    "    game_wrapper=game_wrapper,\n",
    "    policy_model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_step_every=num_eps//num_lr_decay,\n",
    "    num_eps=num_eps,\n",
    "    gamma=gamma,\n",
    "    log_every=log_eps,\n",
    ")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "with open(scores_path, \"w\") as f:\n",
    "    f.writelines([str(x) + \"\\n\" for x in scores])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_agent(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    num_eps: int,\n",
    "    max_steps: int = 1000,\n",
    ") -> Tuple[float, float]:\n",
    "    rewards: List[float] = []\n",
    "\n",
    "    for i_eps in range(num_eps):\n",
    "        game_wrapper = game_wrapper.reset()\n",
    "        state = game_wrapper.get_state()\n",
    "        eps_reward = 0.0\n",
    "        for i_step in range(max_steps):\n",
    "            if i_step == 0:\n",
    "                bet_percent = policy_model.get_bet_percent(state.flatten())\n",
    "                outcome = game_wrapper.bet_step(bet_percent)\n",
    "            else:\n",
    "                card_action = policy_model.get_card_action(state.flatten())\n",
    "                outcome = game_wrapper.card_step(take_card=card_action.item() > random.random())\n",
    "            state = outcome.new_state\n",
    "            terminated = outcome.terminated\n",
    "            eps_reward += outcome.reward\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(eps_reward)\n",
    "\n",
    "    mean_reward = np.mean(rewards).item()\n",
    "    std_reward = np.std(rewards).item()\n",
    "    return mean_reward, std_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "evaluate_agent(\n",
    "    game_wrapper=game_wrapper,\n",
    "    policy_model=model,\n",
    "    num_eps=1000,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
