{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import *\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from game.api import BlackjackWrapper\n",
    "from game.models.model import GameState"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BlackjackPolicyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that accepts a flattened state and outputs 2 values:\n",
    "    1. Bet percentage from 0 to 1\n",
    "    2. Probability of taking a card (hit) from 0 to 1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        # common layers shared by both outputs\n",
    "        self.init_layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        # layers for bet percentage output\n",
    "        self.bet_layers = nn.Sequential(\n",
    "            nn.Linear(32, 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        # layers for card action output\n",
    "        self.card_layers = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.init_layers(x)\n",
    "        x1 = self.bet_layers(x)\n",
    "        x2 = self.card_layers(x)\n",
    "        return x1, x2\n",
    "\n",
    "    def get_bet_percent(self, normalized_state) -> torch.Tensor:\n",
    "        state = torch.from_numpy(normalized_state).float().unsqueeze(0).to(device)\n",
    "        bet_percent, _ = self.forward(state)\n",
    "        return bet_percent.cpu()\n",
    "\n",
    "    def get_card_action(self, normalized_state) -> torch.Tensor:\n",
    "        state = torch.from_numpy(normalized_state).float().unsqueeze(0).to(device)\n",
    "        _, card_prob = self.forward(state)\n",
    "        return card_prob.cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reinforce(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    optimizer: optim.Optimizer,\n",
    "    num_eps: int,\n",
    "    gamma: float,\n",
    "    max_steps: int = 1000,\n",
    "    log_every: int = 10,\n",
    "):\n",
    "    print(\"Starting RL training process...\")\n",
    "    eps_scores: List[float] = []\n",
    "\n",
    "    for i_eps in trange(num_eps):\n",
    "        saved_outputs = []\n",
    "        rewards: List[float] = []\n",
    "        game_wrapper = game_wrapper.reset()\n",
    "        state = game_wrapper.get_state()\n",
    "\n",
    "        for i_step in range(max_steps):\n",
    "            if i_step == 0:\n",
    "                bet_percent = policy_model.get_bet_percent(state.flatten())\n",
    "                saved_outputs.append(bet_percent)\n",
    "                outcome = game_wrapper.bet_step(bet_percent)\n",
    "            else:\n",
    "                card_action = policy_model.get_card_action(state.flatten())\n",
    "                saved_outputs.append(card_action)\n",
    "                outcome = game_wrapper.card_step(take_card=card_action.item() > random.random())\n",
    "            state = outcome.new_state\n",
    "            terminated = outcome.terminated\n",
    "            rewards.append(outcome.reward)\n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "        n_steps = len(rewards)\n",
    "        eps_scores.append(sum(rewards))\n",
    "        returns = deque(maxlen=n_steps)\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = returns[0] if len(returns) > 0 else 0\n",
    "            returns.appendleft(gamma * disc_return_t + rewards[t])\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        # normalize returns\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        model_loss_arr = []\n",
    "        for output, pred_return in zip(saved_outputs, returns):\n",
    "            model_loss_arr.append(-output * pred_return)\n",
    "        model_loss = torch.cat(model_loss_arr).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_eps % log_every == 0:\n",
    "            tqdm.write(f\"Episode {i_eps}\\t\\tRunning Average Score: {round(np.mean(eps_scores).item(), 3)}\")\n",
    "\n",
    "    return eps_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "learning_rate = 1e-3\n",
    "num_eps = 10000\n",
    "log_eps = int(num_eps / 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "game_wrapper = BlackjackWrapper()\n",
    "model = BlackjackPolicyModel(in_features=GameState.get_state_size())\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"blackjack_policy_model\"\n",
    "proj_path = os.path.join(os.getcwd(), \"..\")\n",
    "model_path = os.path.join(proj_path, \"models\", model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = reinforce(\n",
    "    game_wrapper=game_wrapper,\n",
    "    policy_model=model,\n",
    "    optimizer=optimizer,\n",
    "    num_eps=num_eps,\n",
    "    gamma=gamma,\n",
    "    log_every=log_eps,\n",
    ")\n",
    "torch.save(model, model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_agent(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    num_eps: int,\n",
    "    max_steps: int = 1000,\n",
    ") -> Tuple[float, float]:\n",
    "    rewards: List[float] = []\n",
    "\n",
    "    for i_eps in range(num_eps):\n",
    "        game_wrapper = game_wrapper.reset()\n",
    "        state = game_wrapper.get_state()\n",
    "        eps_reward = 0.0\n",
    "        for i_step in range(max_steps):\n",
    "            if i_step == 0:\n",
    "                bet_percent = policy_model.get_bet_percent(state.flatten())\n",
    "                outcome = game_wrapper.bet_step(bet_percent)\n",
    "            else:\n",
    "                card_action = policy_model.get_card_action(state.flatten())\n",
    "                outcome = game_wrapper.card_step(take_card=card_action.item() > random.random())\n",
    "            state = outcome.new_state\n",
    "            terminated = outcome.terminated\n",
    "            eps_reward += outcome.reward\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(eps_reward)\n",
    "\n",
    "    mean_reward = np.mean(rewards).item()\n",
    "    std_reward = np.std(rewards).item()\n",
    "    return mean_reward, std_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluate_agent(\n",
    "    game_wrapper=game_wrapper,\n",
    "    policy_model=model,\n",
    "    num_eps=1000,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
