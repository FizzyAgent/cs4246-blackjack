{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import *\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as distributions\n",
    "\n",
    "from game.api import BlackjackWrapper\n",
    "from game.game_models import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class BlackjackPolicyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model that accepts a flattened state and outputs 2 values:\n",
    "    1. Bet percentage from 0 to 1\n",
    "    2. Probability of taking a card (hit) from 0 to 1\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int):\n",
    "        super().__init__()\n",
    "        # common layers shared by both outputs\n",
    "        self.init_layers = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Linear(64, 16),\n",
    "        )\n",
    "        # layers for bet percentage output\n",
    "        self.bet_layers = nn.Sequential(\n",
    "            nn.Linear(16, 4),\n",
    "            nn.Linear(4, 1),\n",
    "        )\n",
    "        self.bet_act = nn.Sigmoid()\n",
    "        # layers for card action output\n",
    "        self.card_layers = nn.Sequential(\n",
    "            nn.Linear(16, 8),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Linear(4, 1),\n",
    "        )\n",
    "        self.card_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.init_layers(x)\n",
    "        x1 = self.bet_layers(x)\n",
    "        x1 = self.bet_act(x1)\n",
    "        x2 = self.card_layers(x)\n",
    "        x2 = self.card_act(x2)\n",
    "        return x1, x2\n",
    "\n",
    "    def get_bet_percent(self, normalized_state) -> torch.Tensor:\n",
    "        state = torch.from_numpy(normalized_state).float().unsqueeze(0).to(device)\n",
    "        bet_percent, _ = self.forward(state)\n",
    "        return bet_percent.cpu()\n",
    "\n",
    "    def get_card_action(self, normalized_state) -> torch.Tensor:\n",
    "        state = torch.from_numpy(normalized_state).float().unsqueeze(0).to(device)\n",
    "        _, card_prob = self.forward(state)\n",
    "        return card_prob.cpu()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def reinforce(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    optimizer: optim.Optimizer,\n",
    "    num_eps: int,\n",
    "    gamma: float,\n",
    "    log_step: Optional[int] = None\n",
    "):\n",
    "    print(\"Starting RL training process...\")\n",
    "    log_step: int = log_step or max(num_eps // 100, 1)\n",
    "    eps_scores: List[float] = []\n",
    "\n",
    "    for i_episode in trange(num_eps):\n",
    "        saved_outputs = []\n",
    "        rewards: List[float] = []\n",
    "        game_wrapper.reset()\n",
    "        starting_state = True\n",
    "        terminated = False\n",
    "        state = game_wrapper.get_state()\n",
    "\n",
    "        while not terminated:\n",
    "            if starting_state:\n",
    "                bet_percent = policy_model.get_bet_percent(state.flatten())\n",
    "                saved_outputs.append(bet_percent)\n",
    "                outcome = game_wrapper.bet_step(bet_percent)\n",
    "            else:\n",
    "                card_action = policy_model.get_card_action(state.flatten())\n",
    "                saved_outputs.append(card_action)\n",
    "                outcome = game_wrapper.card_step(take_card=card_action.item() > random.random())\n",
    "            state = outcome.new_state\n",
    "            terminated = outcome.terminated\n",
    "            rewards.append(outcome.reward)\n",
    "            starting_state = False\n",
    "\n",
    "        n_steps = len(rewards)\n",
    "        eps_scores.append(sum(rewards))\n",
    "        returns = deque(maxlen=n_steps)\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = returns[0] if len(returns) > 0 else 0\n",
    "            returns.appendleft(gamma * disc_return_t + rewards[t])\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        # normalize returns\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        model_loss_arr = []\n",
    "        for output, pred_return in zip(saved_outputs, returns):\n",
    "            model_loss_arr.append(-output * pred_return)\n",
    "        model_loss = torch.cat(model_loss_arr).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % log_step == 0:\n",
    "            tqdm.write(f\"Episode {i_episode}\\tRunning Average Score: {round(np.mean(eps_scores).item(), 3)}\")\n",
    "\n",
    "    return eps_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "learning_rate = 1e-3\n",
    "num_eps = 10000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "game_wrapper = BlackjackWrapper()\n",
    "model = BlackjackPolicyModel(in_features=GameState.get_state_size())\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RL training process...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2459b00adef54c7f870ed49327aa050b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tRunning Average Score: 1.0\n",
      "Episode 100\tRunning Average Score: 1.0\n",
      "Episode 200\tRunning Average Score: 1.0\n",
      "Episode 300\tRunning Average Score: 1.0\n",
      "Episode 400\tRunning Average Score: 1.0\n",
      "Episode 500\tRunning Average Score: 1.0\n",
      "Episode 600\tRunning Average Score: 1.0\n",
      "Episode 700\tRunning Average Score: 1.0\n",
      "Episode 800\tRunning Average Score: 1.0\n",
      "Episode 900\tRunning Average Score: 1.0\n",
      "Episode 1000\tRunning Average Score: 1.0\n",
      "Episode 1100\tRunning Average Score: 1.0\n",
      "Episode 1200\tRunning Average Score: 1.0\n",
      "Episode 1300\tRunning Average Score: 1.0\n",
      "Episode 1400\tRunning Average Score: 1.0\n",
      "Episode 1500\tRunning Average Score: 1.0\n",
      "Episode 1600\tRunning Average Score: 1.0\n",
      "Episode 1700\tRunning Average Score: 1.0\n",
      "Episode 1800\tRunning Average Score: 1.0\n",
      "Episode 1900\tRunning Average Score: 1.0\n",
      "Episode 2000\tRunning Average Score: 1.0\n",
      "Episode 2100\tRunning Average Score: 1.0\n",
      "Episode 2200\tRunning Average Score: 1.0\n",
      "Episode 2300\tRunning Average Score: 1.0\n",
      "Episode 2400\tRunning Average Score: 1.0\n",
      "Episode 2500\tRunning Average Score: 1.0\n",
      "Episode 2600\tRunning Average Score: 1.0\n",
      "Episode 2700\tRunning Average Score: 1.0\n",
      "Episode 2800\tRunning Average Score: 1.0\n",
      "Episode 2900\tRunning Average Score: 1.0\n",
      "Episode 3000\tRunning Average Score: 1.0\n",
      "Episode 3100\tRunning Average Score: 1.0\n",
      "Episode 3200\tRunning Average Score: 1.0\n",
      "Episode 3300\tRunning Average Score: 1.0\n",
      "Episode 3400\tRunning Average Score: 1.0\n",
      "Episode 3500\tRunning Average Score: 1.0\n",
      "Episode 3600\tRunning Average Score: 1.0\n",
      "Episode 3700\tRunning Average Score: 1.0\n",
      "Episode 3800\tRunning Average Score: 1.0\n",
      "Episode 3900\tRunning Average Score: 1.0\n",
      "Episode 4000\tRunning Average Score: 1.0\n",
      "Episode 4100\tRunning Average Score: 1.0\n",
      "Episode 4200\tRunning Average Score: 1.0\n",
      "Episode 4300\tRunning Average Score: 1.0\n",
      "Episode 4400\tRunning Average Score: 1.0\n",
      "Episode 4500\tRunning Average Score: 1.0\n",
      "Episode 4600\tRunning Average Score: 1.0\n",
      "Episode 4700\tRunning Average Score: 1.0\n",
      "Episode 4800\tRunning Average Score: 1.0\n",
      "Episode 4900\tRunning Average Score: 1.0\n",
      "Episode 5000\tRunning Average Score: 1.0\n",
      "Episode 5100\tRunning Average Score: 1.0\n",
      "Episode 5200\tRunning Average Score: 1.0\n",
      "Episode 5300\tRunning Average Score: 1.0\n",
      "Episode 5400\tRunning Average Score: 1.0\n",
      "Episode 5500\tRunning Average Score: 1.0\n",
      "Episode 5600\tRunning Average Score: 1.0\n",
      "Episode 5700\tRunning Average Score: 1.0\n",
      "Episode 5800\tRunning Average Score: 1.0\n",
      "Episode 5900\tRunning Average Score: 1.0\n",
      "Episode 6000\tRunning Average Score: 1.0\n",
      "Episode 6100\tRunning Average Score: 1.0\n",
      "Episode 6200\tRunning Average Score: 1.0\n",
      "Episode 6300\tRunning Average Score: 1.0\n",
      "Episode 6400\tRunning Average Score: 1.0\n",
      "Episode 6500\tRunning Average Score: 1.0\n",
      "Episode 6600\tRunning Average Score: 1.0\n",
      "Episode 6700\tRunning Average Score: 1.0\n",
      "Episode 6800\tRunning Average Score: 1.0\n",
      "Episode 6900\tRunning Average Score: 1.0\n",
      "Episode 7000\tRunning Average Score: 1.0\n",
      "Episode 7100\tRunning Average Score: 1.0\n",
      "Episode 7200\tRunning Average Score: 1.0\n",
      "Episode 7300\tRunning Average Score: 1.0\n",
      "Episode 7400\tRunning Average Score: 1.0\n",
      "Episode 7500\tRunning Average Score: 1.0\n",
      "Episode 7600\tRunning Average Score: 1.0\n",
      "Episode 7700\tRunning Average Score: 1.0\n",
      "Episode 7800\tRunning Average Score: 1.0\n",
      "Episode 7900\tRunning Average Score: 1.0\n",
      "Episode 8000\tRunning Average Score: 1.0\n",
      "Episode 8100\tRunning Average Score: 1.0\n",
      "Episode 8200\tRunning Average Score: 1.0\n",
      "Episode 8300\tRunning Average Score: 1.0\n",
      "Episode 8400\tRunning Average Score: 1.0\n",
      "Episode 8500\tRunning Average Score: 1.0\n",
      "Episode 8600\tRunning Average Score: 1.0\n",
      "Episode 8700\tRunning Average Score: 1.0\n",
      "Episode 8800\tRunning Average Score: 1.0\n",
      "Episode 8900\tRunning Average Score: 1.0\n",
      "Episode 9000\tRunning Average Score: 1.0\n",
      "Episode 9100\tRunning Average Score: 1.0\n",
      "Episode 9200\tRunning Average Score: 1.0\n",
      "Episode 9300\tRunning Average Score: 1.0\n",
      "Episode 9400\tRunning Average Score: 1.0\n",
      "Episode 9500\tRunning Average Score: 1.0\n",
      "Episode 9600\tRunning Average Score: 1.0\n",
      "Episode 9700\tRunning Average Score: 1.0\n",
      "Episode 9800\tRunning Average Score: 1.0\n",
      "Episode 9900\tRunning Average Score: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n 1.0,\n ...]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reinforce(\n",
    "    game_wrapper=game_wrapper,\n",
    "    policy_model=model,\n",
    "    optimizer=optimizer,\n",
    "    num_eps=num_eps,\n",
    "    gamma=gamma,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
