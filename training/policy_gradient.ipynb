{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import *\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from game.api import BlackjackWrapper\n",
    "from game.models.model import GameState\n",
    "from training.agent import BlackjackPolicyModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def reinforce(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler: optim.lr_scheduler._LRScheduler,\n",
    "    scheduler_step_every: int,\n",
    "    num_eps: int,\n",
    "    batch_size: int,\n",
    "    gamma: float,\n",
    "    max_steps: int = 10,\n",
    "    add_step: bool = False,\n",
    "    add_card_counting: bool = True,\n",
    "    log_every: int = 10,\n",
    "):\n",
    "    print(\"Starting RL training process...\")\n",
    "    all_scores: List[List[float]] = []\n",
    "    eps_scores: List[float] = []\n",
    "    last_logged_eps_scores: List[float] = []\n",
    "\n",
    "    for i_eps in trange(num_eps):\n",
    "        model_loss = []\n",
    "        batch_returns = []\n",
    "        batch_outputs = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            outputs = []\n",
    "            rewards: List[float] = []\n",
    "            game_wrapper = game_wrapper.reset()\n",
    "            state = game_wrapper.get_state()\n",
    "            for i_step in range(max_steps):\n",
    "                step_num = i_step / max_steps if add_step else None\n",
    "                state_features = state.flatten(include_discarded=add_card_counting, step_num=step_num)\n",
    "                if i_step == 0:\n",
    "                    bet_percent = policy_model.get_bet_percent(state_features)\n",
    "                    outcome = game_wrapper.bet_step(bet_percent)\n",
    "                    outputs.append(bet_percent)\n",
    "                else:\n",
    "                    card_action = policy_model.get_card_action(state_features)\n",
    "                    outputs.append(card_action)\n",
    "                    take_card = card_action.item() > random.random()\n",
    "                    outcome = game_wrapper.card_step(take_card=take_card)\n",
    "                state = outcome.new_state\n",
    "                terminated = outcome.terminated\n",
    "                rewards.append(outcome.reward)\n",
    "                if terminated:\n",
    "                    break\n",
    "\n",
    "            n_steps = len(rewards)\n",
    "            eps_reward = sum(rewards)\n",
    "            all_scores.append(rewards)\n",
    "            eps_scores.append(eps_reward)\n",
    "            last_logged_eps_scores.append(eps_reward)\n",
    "            returns = deque(maxlen=n_steps)\n",
    "\n",
    "            for t in range(n_steps)[::-1]:\n",
    "                disc_return_t = returns[0] if len(returns) > 0 else 0\n",
    "                returns.appendleft(gamma * disc_return_t + rewards[t])\n",
    "            returns = np.array(returns)\n",
    "\n",
    "            batch_returns.extend(returns)\n",
    "            batch_outputs.extend(outputs)\n",
    "\n",
    "        batch_returns = np.array(batch_returns)\n",
    "        batch_returns = (batch_returns - batch_returns.mean()) / (batch_returns.std() + 1e-8)\n",
    "        for pred_return, output in zip(batch_returns, batch_outputs):\n",
    "            model_loss.append(-pred_return * (output + 1e-8).log())\n",
    "\n",
    "        model_loss = torch.cat(model_loss)\n",
    "        model_loss = model_loss.sum()\n",
    "        optimizer.zero_grad()\n",
    "        model_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_eps % scheduler_step_every == 0:\n",
    "            scheduler.step()\n",
    "\n",
    "        if i_eps % log_every == 0:\n",
    "            tqdm.write(\n",
    "                f\"Episode {i_eps}\"\n",
    "                f\"\\t\\tLast Logged Average Score: {round(np.mean(last_logged_eps_scores).item(), 3)}\"\n",
    "                f\"\\t\\tRunning Average Score: {round(np.mean(eps_scores).item(), 3)}\"\n",
    "            )\n",
    "            last_logged_eps_scores = []\n",
    "\n",
    "    return all_scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "learning_rate = 1e-2\n",
    "batch_size = 8\n",
    "num_eps = 10_000\n",
    "lr_gamma = 0.99\n",
    "num_lr_decay = 100\n",
    "log_eps = int(num_eps / 100)\n",
    "add_steps_info = True\n",
    "add_card_counting = True\n",
    "\n",
    "initial_cash = 1000\n",
    "deck_num = 8\n",
    "min_bet = 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "game_wrapper = BlackjackWrapper(initial_cash, deck_num, min_bet)\n",
    "in_features = GameState.get_state_size(add_steps=add_steps_info)\n",
    "model = BlackjackPolicyModel(in_features=in_features, device=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_gamma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"blackjack_policy_model_eps10000_batch8\"\n",
    "proj_path = os.path.join(os.getcwd(), \"..\")\n",
    "# proj_path = os.getcwd()\n",
    "model_dir = os.path.join(proj_path, \"models\")\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "scores_path = os.path.join(model_dir, f\"{model_name}_training_scores\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train()\n",
    "scores = reinforce(\n",
    "    game_wrapper=game_wrapper,\n",
    "    policy_model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    scheduler_step_every=num_eps//num_lr_decay,\n",
    "    num_eps=num_eps,\n",
    "    batch_size=batch_size,\n",
    "    gamma=gamma,\n",
    "    add_step=add_steps_info,\n",
    "    add_card_counting=add_card_counting,\n",
    "    log_every=log_eps,\n",
    ")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "with open(scores_path, \"w\") as f:\n",
    "    f.writelines([\", \".join([str(x) for x in arr]) + \"\\n\" for arr in scores])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_agent(\n",
    "    game_wrapper: BlackjackWrapper,\n",
    "    policy_model: BlackjackPolicyModel,\n",
    "    num_eps: int,\n",
    "    max_steps: int = 10,\n",
    "    add_step: bool = False,\n",
    "    add_card_counting: bool = True,\n",
    ") -> Tuple[float, float]:\n",
    "    rewards: List[float] = []\n",
    "\n",
    "    for i_eps in range(num_eps):\n",
    "        game_wrapper = game_wrapper.reset()\n",
    "        state = game_wrapper.get_state()\n",
    "        eps_reward = 0.0\n",
    "        for i_step in range(max_steps):\n",
    "            step_num = i_step / max_steps if add_step else None\n",
    "            state_features = state.flatten(include_discarded=add_card_counting, step_num=step_num)\n",
    "            if i_step == 0:\n",
    "                bet_percent = policy_model.get_bet_percent(state_features)\n",
    "                outcome = game_wrapper.bet_step(bet_percent)\n",
    "            else:\n",
    "                card_action = policy_model.get_card_action(state_features)\n",
    "                outcome = game_wrapper.card_step(take_card=card_action.item() > random.random())\n",
    "            state = outcome.new_state\n",
    "            terminated = outcome.terminated\n",
    "            eps_reward += outcome.reward\n",
    "            if terminated:\n",
    "                break\n",
    "        rewards.append(eps_reward)\n",
    "\n",
    "    mean_reward = np.mean(rewards).item()\n",
    "    std_reward = np.std(rewards).item()\n",
    "    return mean_reward, std_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "evaluate_agent(\n",
    "    game_wrapper=game_wrapper,\n",
    "    policy_model=model,\n",
    "    num_eps=1000,\n",
    "    add_step=add_steps_info,\n",
    "    add_card_counting=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
